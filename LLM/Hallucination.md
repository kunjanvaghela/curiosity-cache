
- Generated text that is non-factual and/or ungrounded
- Can reduce hallucination based on few methods

Can counter using:
1. RAG
2. Fine Tuning

>[!tip] There are no ways to stop hallucinations

### Groundedness and Attributability
- Generated text from an LLM is grounded in a document if the document supports the text.
- The TRUE model: for measuring groundedness via NLI
- Can train an LLM to output sentences with citations.
