
- Works by decomposing the weight matrices of the LLM into the low-rank matrices.
	- This reduces the number of parameters that need to be trained, while still maintaining the performance of the original model.


Concepts used:
1. [[Self Attention]]
2. 


<iframe width="560" height="315" src="https://www.youtube.com/embed/KEv-F5UkhxU?si=crRKKNI3zL8UEATU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>



## References
1. LoRA paper: https://arxiv.org/pdf/2106.09685
2. https://medium.com/datadreamers/exploring-lora-unveiling-parameter-efficient-tuning-and-self-attention-mechanisms-in-depth-58e4c3b5ce30#:~:text=It%20is%20a%20technique%20for,process%20faster%20and%20more%20efficient.
3. Paper and Method Explanation: https://www.youtube.com/watch?v=dA-NhCtrrVE